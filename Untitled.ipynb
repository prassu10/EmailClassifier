{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64e380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43097abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef2c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "\n",
    "import string\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87eda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_DIR = os.path.join(\"archive/\")\n",
    "SPAM_DIR = os.path.join(EMAIL_DIR, \"spam\")\n",
    "HAM_DIR = os.path.join(EMAIL_DIR, \"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe3767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c9a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails_list(file_dir, tag, proportion=1):\n",
    "    files = os.listdir(file_dir)\n",
    "    files_length = int(len(files)*proportion)\n",
    "    files = files[:files_length]\n",
    "    tag_list = []\n",
    "    for a_file in files:\n",
    "        if not a_file.startswith(\".\"):\n",
    "            with codecs.open(os.path.join(file_dir, a_file), \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\") as f:\n",
    "                email = f.read()\n",
    "        tag_list.append((email, tag))\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2a7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17156\n",
      "(\"Subject: what up , , your cam babe\\r\\nwhat are you looking for ?\\r\\nif your looking for a companion for friendship , love , a date , or just good ole '\\r\\nfashioned * * * * * * , then try our brand new site ; it was developed and created\\r\\nto help anyone find what they ' re looking for . a quick bio form and you ' re\\r\\non the road to satisfaction in every sense of the word . . . . no matter what\\r\\nthat may be !\\r\\ntry it out and youll be amazed .\\r\\nhave a terrific time this evening\\r\\ncopy and pa ste the add . ress you see on the line below into your browser to come to the site .\\r\\nhttp : / / www . meganbang . biz / bld / acc /\\r\\nno more plz\\r\\nhttp : / / www . naturalgolden . com / retract /\\r\\ncounterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\\r\\n\", 'spam')\n"
     ]
    }
   ],
   "source": [
    "spam_list = get_emails_list(SPAM_DIR, \"spam\", 1)\n",
    "print(len(spam_list))\n",
    "print(spam_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7f21c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16545\n",
      "(\"Subject: ena sales on hpl\\r\\njust to update you on this project ' s status :\\r\\nbased on a new report that scott mills ran for me from sitara , i have come up\\r\\nwith the following counterparties as the ones to which ena is selling gas off\\r\\nof hpl ' s pipe .\\r\\naltrade transaction , l . l . c . gulf gas utilities company\\r\\nbrazoria , city of panther pipeline , inc .\\r\\ncentral illinois light company praxair , inc .\\r\\ncentral power and light company reliant energy - entex\\r\\nces - equistar chemicals , lp reliant energy - hl & p\\r\\ncorpus christi gas marketing , lp southern union company\\r\\nd & h gas company , inc . texas utilities fuel company\\r\\nduke energy field services , inc . txu gas distribution\\r\\nentex gas marketing company union carbide corporation\\r\\nequistar chemicals , lp unit gas transmission company inc .\\r\\nsince i ' m not sure exactly what gets entered into sitara , pat clynes\\r\\nsuggested that i check with daren farmer to make sure that i ' m not missing\\r\\nsomething ( which i did below ) . while i am waiting for a response from him\\r\\nand / or mary smith , i will begin gathering the contractual volumes under the\\r\\nabove contracts .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by cheryl dudley / hou / ect on 05 / 10 / 2000 07 : 56\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\ncheryl d king\\r\\n05 / 08 / 2000 04 : 11 pm\\r\\nsent by : cheryl dudley\\r\\nto : daren j farmer / hou / ect @ ect , mary m smith / hou / ect @ ect\\r\\ncc :\\r\\nsubject : ena sales on hpl\\r\\ni am working on a project for brenda herod & was wondering if one of you\\r\\ncould tell me if i ' m on the right track & if this will get everything for\\r\\nwhich she is looking .\\r\\nshe is trying to draft a long - term transport / storage agreement between ena &\\r\\nhplc which will allow ena to move the gas to their markets . in order to\\r\\naccomplish this , she needs to know all of the sales to customers that ena is\\r\\ndoing off of hpl ' s pipe .\\r\\ni had scott mills run a report from sitara showing all ena buy / sell activity\\r\\non hpl since 7 / 99 . if i eliminate the buys & the desk - to - desk deals , will\\r\\nthis give me everything that i need ?\\r\\nare there buy / sell deals done with ena on hpl ' s pipe that wouldn ' t show up in\\r\\nsitara ? someone mentioned something about deals where hpl transports the gas\\r\\non it ' s own behalf then ena sells it to a customer at that same spot - -\\r\\n? ? ? ? ? do deals like that happen ? would they show up in sitara ?\\r\\nis there anything else that i ' m missing ? i ' m not real familiar with how some\\r\\nof these deals happen nowadays so am very receptive to any\\r\\nideas / suggestions / help that you can offer ! ! !\\r\\nthanks in advance .\", 'ham')\n"
     ]
    }
   ],
   "source": [
    "ham_list = get_emails_list(HAM_DIR, \"ham\", 1)\n",
    "print(len(ham_list))\n",
    "print(ham_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05fc24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33701\n"
     ]
    }
   ],
   "source": [
    "email_list = spam_list + ham_list\n",
    "print(len(email_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2641c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df = pd.DataFrame(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320484fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=2, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(email_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae0e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df.rename(columns = {0:'message'}, inplace = True)\n",
    "email_df.rename(columns = {1:'category'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccceae33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['message', 'category'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12976ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                  message category\n",
       "0      Subject: what up , , your cam babe\\r\\nwhat are...     spam\n",
       "1      Subject: do you know wwhat ?\\r\\nhello , welcom...     spam\n",
       "2      Subject: friend , never be in pain again\\r\\nis...     spam\n",
       "3      Subject: big range of all types of downloadabl...     spam\n",
       "4      Subject: software\\r\\nmicrosoft windows xp prof...     spam\n",
       "...                                                  ...      ...\n",
       "33696  Subject: teco update\\r\\nwe received their redr...      ham\n",
       "33697  Subject: \" project doorstep \" target date\\r\\nt...      ham\n",
       "33698  Subject: re : a note being sent out under john...      ham\n",
       "33699  Subject: associate & analyst mid - year 2001 p...      ham\n",
       "33700  Subject: re : visit to enron\\r\\nfyi\\r\\n- - - -...      ham\n",
       "\n",
       "[33701 rows x 2 columns]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d1b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df.loc[email_df['category'] == 'spam', 'category',] = 0\n",
    "email_df.loc[email_df['category'] == 'ham', 'category',] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4881e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Subject: what up , , your cam babe\\r\\nwhat are...\n",
      "1        Subject: do you know wwhat ?\\r\\nhello , welcom...\n",
      "2        Subject: friend , never be in pain again\\r\\nis...\n",
      "3        Subject: big range of all types of downloadabl...\n",
      "4        Subject: software\\r\\nmicrosoft windows xp prof...\n",
      "                               ...                        \n",
      "33696    Subject: teco update\\r\\nwe received their redr...\n",
      "33697    Subject: \" project doorstep \" target date\\r\\nt...\n",
      "33698    Subject: re : a note being sent out under john...\n",
      "33699    Subject: associate & analyst mid - year 2001 p...\n",
      "33700    Subject: re : visit to enron\\r\\nfyi\\r\\n- - - -...\n",
      "Name: message, Length: 33701, dtype: object\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "33696    1\n",
      "33697    1\n",
      "33698    1\n",
      "33699    1\n",
      "33700    1\n",
      "Name: category, Length: 33701, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = email_df['message']\n",
    "\n",
    "Y = email_df['category']\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61947aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33701,)\n",
      "(26960,)\n",
      "(6741,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training data & test data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a5180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c262d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16333    Subject: re : re : good\\r\\nthe museum of the f...\n",
      "26175    Subject: stats\\r\\nwest power desk\\r\\nassuming ...\n",
      "25748    Subject: re : uk rab multiples\\r\\nvince ,\\r\\nw...\n",
      "16620    Subject: largest pornstars collection of downl...\n",
      "25941    Subject: today is the signing deadline for ful...\n",
      "                               ...                        \n",
      "25365    Subject: schedule crawler : hourahead failure\\...\n",
      "25544    Subject: hpl fuel gas buy - back for december ...\n",
      "11513    Subject: penny flyer showcasing increased cont...\n",
      "1688     Subject: did you know : america is giving away...\n",
      "5994     Subject: sometimes less is more . . .\\r\\nfind ...\n",
      "Name: message, Length: 26960, dtype: object\n",
      "  (0, 108101)\t0.09075106642955155\n",
      "  (0, 111626)\t0.11114671876086879\n",
      "  (0, 102980)\t0.08416389416151358\n",
      "  (0, 69597)\t0.16499466291247547\n",
      "  (0, 106106)\t0.1735079509612841\n",
      "  (0, 29473)\t0.11327993172314248\n",
      "  (0, 130093)\t0.10154111247295709\n",
      "  (0, 121759)\t0.1271113575295443\n",
      "  (0, 36413)\t0.16015230546445677\n",
      "  (0, 61153)\t0.1848334970276382\n",
      "  (0, 137592)\t0.16796485725302612\n",
      "  (0, 101651)\t0.14207361016721787\n",
      "  (0, 113798)\t0.15911053156630572\n",
      "  (0, 124867)\t0.08319320630642894\n",
      "  (0, 113365)\t0.16796485725302612\n",
      "  (0, 98851)\t0.0743627690868334\n",
      "  (0, 41108)\t0.1452543871888087\n",
      "  (0, 95873)\t0.10899918155745883\n",
      "  (0, 14551)\t0.16125364084288332\n",
      "  (0, 91436)\t0.1848334970276382\n",
      "  (0, 121537)\t0.12327987461996269\n",
      "  (0, 29792)\t0.08697230071708506\n",
      "  (0, 101718)\t0.14337307709634756\n",
      "  (0, 70407)\t0.08156767560697083\n",
      "  (0, 134803)\t0.14679665996762942\n",
      "  :\t:\n",
      "  (26959, 36399)\t0.05552882397716442\n",
      "  (26959, 114326)\t0.08500177390015602\n",
      "  (26959, 68836)\t0.05642773237481775\n",
      "  (26959, 78402)\t0.03843510307918097\n",
      "  (26959, 134699)\t0.03501307010871575\n",
      "  (26959, 77942)\t0.13330239827073143\n",
      "  (26959, 20665)\t0.03818654079824035\n",
      "  (26959, 137906)\t0.03556726992221895\n",
      "  (26959, 98026)\t0.04055368358024865\n",
      "  (26959, 55069)\t0.08891480227154014\n",
      "  (26959, 80921)\t0.033881390389798274\n",
      "  (26959, 126781)\t0.04625375547751187\n",
      "  (26959, 114096)\t0.03319564522655119\n",
      "  (26959, 83837)\t0.03173013206302922\n",
      "  (26959, 44009)\t0.038062059081446976\n",
      "  (26959, 96260)\t0.04570345391136416\n",
      "  (26959, 0)\t0.1406923217333544\n",
      "  (26959, 137219)\t0.037585818321046856\n",
      "  (26959, 68201)\t0.03092149726353662\n",
      "  (26959, 126004)\t0.035887664170346785\n",
      "  (26959, 64972)\t0.030113791143605403\n",
      "  (26959, 45798)\t0.06498970791665613\n",
      "  (26959, 43812)\t0.04106611481242768\n",
      "  (26959, 33829)\t0.025786826123190634\n",
      "  (26959, 120730)\t0.011358842974552051\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# transform the text data to feature vectors that can be used as input to the Logistic regression\n",
    "\n",
    "feature_extraction = TfidfVectorizer(min_df = 1, stop_words='english', lowercase='True')\n",
    "\n",
    "X_train_features = feature_extraction.fit_transform(X_train)\n",
    "X_test_features = feature_extraction.transform(X_test)\n",
    "\n",
    "# convert Y_train and Y_test values as integers\n",
    "\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "print(X_train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fab95b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# training the Logistic Regression model with the training data\n",
    "model.fit(X_train_features, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7f85543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data :  0.9924703264094955\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model\n",
    "\n",
    "# prediction on training data\n",
    "\n",
    "prediction_on_training_data = model.predict(X_train_features)\n",
    "accuracy_on_training_data = accuracy_score(Y_train, prediction_on_training_data)\n",
    "\n",
    "print('Accuracy on training data : ', accuracy_on_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d97ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data :  0.9860554813825841\n"
     ]
    }
   ],
   "source": [
    "# prediction on test data\n",
    "\n",
    "prediction_on_test_data = model.predict(X_test_features)\n",
    "accuracy_on_test_data = accuracy_score(Y_test, prediction_on_test_data)\n",
    "\n",
    "print('Accuracy on test data : ', accuracy_on_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc1cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Spam mail\n",
      "1 Ham mail\n",
      "1 Ham mail\n",
      "0 Spam mail\n",
      "1 Ham mail\n",
      "0 Spam mail\n"
     ]
    }
   ],
   "source": [
    "# Building a Predictive System\n",
    "\n",
    "test_mail_list = [\"Participate in our new lottery!\", \n",
    "                  \"See the minutes from the last meeting attached\", \n",
    "                  \"Investors are coming to our office on Monday\", \n",
    "                  \"Try out this new medicine\",\n",
    "                  \"\"\"\n",
    "                     Subject: confidential folder to safely pass information to arthur andersen\n",
    "                     we have become increasingly concerned about confidential information ( dpr / position info , curves , validations / stress tests , etc ) being passed to arthur andersen for audit purposes over the web to their arthur andersen email addresses . ( necessary now they no longer have access to enron ' s internal email system )\n",
    "                     please use the folder described below when passing any info ( that you would have concerns about if it was picked up by a third party ) via the shared drive that has been set up for this specific purpose .\n",
    "                     note : aa should also use the shared drive to pass info back if there are questions , or the data needs updating . we should also consider the sensitivity of audit findings and special presentations if they are being distributed electronically .\n",
    "                     please pass this note to others in your groups who have the need to pass info back and forth .\n",
    "                     details on how to access for those who will use this method to pass info :\n",
    "                     a secured folder has been set up on the \" o \" drive under corporate called arthur _ andersen ( o : \\ corporate \\ arthur _ anderson ) . please post all confidential files in this folder rather than emailing the files to their company email address . if you need access to this folder , submit an erequest through the it central site : http : / / itcentral . enron . com / data / services / securityrequests / . arthur andersen will be able to retrieve these files for review with their terminal server access at the three allen center location .\n",
    "                     please contact vanessa schulte if you have any problems or questions\n",
    "                     beth apollo\n",
    "                  \"\"\",\n",
    "                  \"\"\"\n",
    "                     Subject: yukos oil\n",
    "                     dear friend ,\n",
    "                     i am mr olsom berghart a personal treasurer to mikhail khodorkovsky the richest man in russia and owner of the following companies : chairman ceo : yukos oil ( russian most largest oil company ) chairman ceo : menatep sbp bank ( a well reputable financial institution with its branches all over the world )\n",
    "                     source of funds :\n",
    "                     i have a profiling amount in an excess of us $ 100 , 500 , 000 which i seek your partnership in accommodating for me . you will be rewarded with 4 % of the total sum for your partnership . can you be my partner on this ?\n",
    "                     introduction of my self\n",
    "                     as a personal consultant to him , authority was handed over to me in transferring money of an american oil merchant for his last oil deal with my boss mikhail khodorkovsky . already the funds have left the shore of russia to a european private bank where\n",
    "                     the final crediting is expected to be carried out . while i was on the process , my boss got arrested for his involvement in politics by financing the leading and opposing political parties ( the union of right forces , led by boris nemtsov , and yabloko , a liberal / social democratic party led by gregor yavlinsky ) which poses treat to president vladimir putin second tenure as russian president . you can catch more of the story on the following website :\n",
    "                     your role :\n",
    "                     all i need from you is to stand as the beneficiary of the above quoted sum and i will re - profile the funds with your name , which will enable the european bank transfer the sum to you . i have decided to use this sum to relocate to your country and never to\n",
    "                     be connected to any of mikhail khodorkovsky conglomerates . the transaction has to be concluded before my boss is out from jail . as soon as i confirm your readiness to conclude the transaction with me , i will provide you with the details .\n",
    "                     thank you very much\n",
    "                     regards\n",
    "                     olsom berghart ( mr )\n",
    "                     mail sent from webmail service at php - nuke powered site\n",
    "                     - http : / / yoursite . com\n",
    "                  \"\"\"\n",
    "                 ]\n",
    "\n",
    "for input_mail in test_mail_list:\n",
    "    input_mail=[input_mail]\n",
    "    # convert text to feature vectors\n",
    "    input_data_features = feature_extraction.transform(input_mail)\n",
    "\n",
    "    # making prediction\n",
    "\n",
    "    prediction = model.predict(input_data_features)\n",
    "    print(int(prediction),end=' ')\n",
    "\n",
    "\n",
    "    if (prediction[0]==1):\n",
    "      print('Ham mail')\n",
    "\n",
    "    else:\n",
    "      print('Spam mail')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dcd68c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral Sentiment\n",
      "Neutral Sentiment ['attached']\n",
      "Neutral Sentiment\n",
      "Neutral Sentiment\n",
      "Positive Sentiment ['attracted']\n",
      "Positive Sentiment ['attached']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def sentiment_analyse(sentiment_text):\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(sentiment_text)\n",
    "    if score['neg'] > score['pos']:\n",
    "        print(\"Negative Sentiment\",end=' ')\n",
    "    elif score['neg'] < score['pos']:\n",
    "        print(\"Positive Sentiment\",end=' ')\n",
    "    else:\n",
    "        print(\"Neutral Sentiment\",end=' ')\n",
    "\n",
    "\n",
    "for input_mail in test_mail_list:\n",
    "    text = input_mail\n",
    "    lower_case = text.lower()\n",
    "    cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Using word_tokenize because it's faster than split()\n",
    "    tokenized_words = word_tokenize(cleaned_text, \"english\")\n",
    "\n",
    "    # Removing Stop Words\n",
    "    final_words = []\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            final_words.append(word)\n",
    "\n",
    "    # Lemmatization - From plural to single + Base form of a word (example better-> good)\n",
    "    lemma_words = []\n",
    "    for word in final_words:\n",
    "        word = WordNetLemmatizer().lemmatize(word)\n",
    "        lemma_words.append(word)\n",
    "\n",
    "    emotion_list = []\n",
    "    with open('emotions.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            clear_line = line.replace(\"\\n\", '').replace(\",\", '').replace(\"'\", '').strip()\n",
    "            word, emotion = clear_line.split(': ')\n",
    "\n",
    "            if word in lemma_words:\n",
    "                emotion_list.append(emotion)\n",
    "\n",
    "    if not emotion_list:\n",
    "        print(\"Neutral Sentiment\")\n",
    "    else:\n",
    "        sentiment_analyse(cleaned_text)\n",
    "        print(emotion_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a18e921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral']\n",
      "['Forward']\n",
      "Neutral Action\n",
      "Neutral Action\n",
      "['Reply', 'Forward']\n",
      "['Reply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/prashanth/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "for input_mail in test_mail_list:\n",
    "    text = input_mail\n",
    "    lower_case = text.lower()\n",
    "    cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Using word_tokenize because it's faster than split()\n",
    "    tokenized_words = word_tokenize(cleaned_text, \"english\")\n",
    "\n",
    "    # Removing Stop Words\n",
    "    final_words = []\n",
    "    for word in tokenized_words:\n",
    "        final_words.append(word)\n",
    "\n",
    "    # Lemmatization - From plural to single + Base form of a word (example better-> good)\n",
    "    lemma_words = []\n",
    "    for word in final_words:\n",
    "        word = WordNetLemmatizer().lemmatize(word)\n",
    "        lemma_words.append(word)\n",
    "\n",
    "    action_list = []\n",
    "    with open('action.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            clear_line = line.replace(\"\\n\", '').replace(\",\", '').replace(\"'\", '').strip()\n",
    "            word, action = clear_line.split(': ')\n",
    "            word, action = word.lower(), action.capitalize()\n",
    "            if word in lemma_words:\n",
    "                if action not in action_list:\n",
    "                    action_list.append(action)\n",
    "\n",
    "    if not action_list:\n",
    "        print(\"Neutral Action\")\n",
    "    else:\n",
    "        print(action_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d06329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c416a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c7120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0b679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
